# @Author: Bi Ying
# @Date:   2024-07-10 17:22:55
import shutil
import os
from pathlib import Path
from typing import Union
import logging

import torch
import torchaudio
import numpy as np
from funasr import AutoModel
from fastapi import FastAPI, Form, UploadFile, File, HTTPException, status

# é…ç½®ç®¡ç†ç±»
class Config:
    """åº”ç”¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        # æœåŠ¡å™¨é…ç½®
        self.HOST = os.getenv("SENSEVOICE_HOST", "0.0.0.0")
        self.PORT = int(os.getenv("SENSEVOICE_PORT", "8000"))
        
        # æ¨¡å‹é…ç½®
        self.DEVICE = os.getenv("SENSEVOICE_DEVICE", "cpu")
        self.CACHE_DIR = os.getenv("SENSEVOICE_CACHE_DIR") or os.path.expanduser("~/.cache/modelscope/hub/models")
        self.MAIN_MODEL = os.getenv("SENSEVOICE_MAIN_MODEL", "iic/SenseVoiceSmall")
        self.VAD_MODEL = os.getenv("SENSEVOICE_VAD_MODEL", "iic/speech_fsmn_vad_zh-cn-16k-common-pytorch")
        
        # æ–‡ä»¶å¤„ç†é…ç½®
        self.TMP_DIR = os.getenv("SENSEVOICE_TMP_DIR", "./tmp")
        self.MAX_FILE_SIZE = int(os.getenv("SENSEVOICE_MAX_FILE_SIZE", str(25 * 1024 * 1024)))  # 25MB
        self.SUPPORTED_FORMATS = {'.mp3', '.mp4', '.mpeg', '.mpga', '.m4a', '.wav', '.webm', '.flac', '.ogg'}
        
        # éŸ³é¢‘å¤„ç†é…ç½®
        self.TARGET_SAMPLE_RATE = int(os.getenv("SENSEVOICE_SAMPLE_RATE", "16000"))
        self.VAD_MAX_SEGMENT_TIME = int(os.getenv("SENSEVOICE_VAD_MAX_SEGMENT", "30000"))
        
        # æ—¥å¿—é…ç½®
        self.LOG_LEVEL = os.getenv("SENSEVOICE_LOG_LEVEL", "INFO")
        
        # APIé…ç½®
        self.API_TITLE = os.getenv("SENSEVOICE_API_TITLE", "SenseVoice OpenAI Compatible API")
        self.API_VERSION = os.getenv("SENSEVOICE_API_VERSION", "1.0.1")
        
        # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
        os.makedirs(self.TMP_DIR, exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=getattr(logging, self.LOG_LEVEL.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(os.path.join(self.TMP_DIR, 'api.log'))
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_model_paths(self):
        """è·å–æ¨¡å‹è·¯å¾„é…ç½®"""
        # ä¸»æ¨¡å‹è·¯å¾„
        if self.MAIN_MODEL.startswith("/") or self.MAIN_MODEL.startswith("~"):
            main_model_path = os.path.expanduser(self.MAIN_MODEL)
        else:
            main_model_path = str(Path(self.CACHE_DIR) / self.MAIN_MODEL)
        
        # VADæ¨¡å‹è·¯å¾„  
        if self.VAD_MODEL.startswith("/") or self.VAD_MODEL.startswith("~"):
            vad_model_path = os.path.expanduser(self.VAD_MODEL)
        else:
            vad_model_path = str(Path(self.CACHE_DIR) / self.VAD_MODEL)
        
        return main_model_path, vad_model_path
    
    def print_config(self):
        """æ‰“å°å½“å‰é…ç½®"""
        print("ğŸ”§ SenseVoice API é…ç½®ï¼š")
        print(f"   æœåŠ¡åœ°å€: http://{self.HOST}:{self.PORT}")
        print(f"   è®¡ç®—è®¾å¤‡: {self.DEVICE}")
        print(f"   ä¸´æ—¶ç›®å½•: {self.TMP_DIR}")
        print(f"   æœ€å¤§æ–‡ä»¶: {self.MAX_FILE_SIZE // (1024*1024)}MB")
        print(f"   æ—¥å¿—çº§åˆ«: {self.LOG_LEVEL}")
        main_path, vad_path = self.get_model_paths()
        print(f"   ä¸»æ¨¡å‹: {main_path}")
        print(f"   VADæ¨¡å‹: {vad_path}")

# åˆå§‹åŒ–é…ç½®
config = Config()
config.print_config()

# FastAPIåº”ç”¨åˆå§‹åŒ–
app = FastAPI(
    title=config.API_TITLE,
    version=config.API_VERSION,
    description="SenseVoiceè¯­éŸ³è½¬æ–‡å­—æœåŠ¡ï¼Œå…¼å®¹OpenAI Whisper API"
)

# ä¸´æ—¶ç›®å½•å˜é‡ (å‘åå…¼å®¹)
TMP_DIR = config.TMP_DIR


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "message": config.API_TITLE, 
        "version": config.API_VERSION,
        "compatibility": "OpenAI Whisper API v1",
        "endpoints": {
            "transcriptions": "/v1/audio/transcriptions",
            "models": "/v1/models",
            "health": "/health",
            "docs": "/docs"
        },
        "supported_formats": sorted(list(config.SUPPORTED_FORMATS)),
        "max_file_size_mb": config.MAX_FILE_SIZE // (1024 * 1024),
        "device": config.DEVICE
    }


@app.get("/health")
async def health():
    """å¢å¼ºçš„å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    import psutil
    import time
    
    start_time = time.time()
    
    # åŸºç¡€çŠ¶æ€æ£€æŸ¥
    health_status = {
        "status": "healthy",
        "timestamp": int(time.time()),
        "uptime": time.time() - app_start_time if 'app_start_time' in globals() else 0,
        "version": config.API_VERSION
    }
    
    # æ¨¡å‹çŠ¶æ€æ£€æŸ¥
    if model is None:
        health_status["status"] = "unhealthy"
        health_status["model_status"] = "unavailable"
        health_status["issues"] = ["Model not loaded"]
    else:
        health_status["model_status"] = "available"
    
    # ç³»ç»Ÿèµ„æºæ£€æŸ¥
    try:
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ç©ºé—´ï¼ˆä¸´æ—¶ç›®å½•ï¼‰
        disk = psutil.disk_usage(config.TMP_DIR)
        
        health_status["system"] = {
            "cpu_percent": cpu_percent,
            "memory": {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "percent": memory.percent
            },
            "disk": {
                "total_gb": round(disk.total / (1024**3), 2),
                "free_gb": round(disk.free / (1024**3), 2),
                "percent": round((disk.used / disk.total) * 100, 1)
            }
        }
        
        # èµ„æºè­¦å‘Šæ£€æŸ¥
        issues = []
        if cpu_percent > 90:
            issues.append("High CPU usage")
        if memory.percent > 90:
            issues.append("High memory usage")
        if disk.free < 1024**3:  # å°äº1GBå¯ç”¨ç©ºé—´
            issues.append("Low disk space")
            
        if issues:
            health_status["status"] = "degraded"
            health_status["issues"] = issues
    
    except Exception as e:
        health_status["status"] = "degraded"
        health_status["issues"] = [f"System monitoring error: {str(e)}"]
    
    # é…ç½®ä¿¡æ¯
    health_status["config"] = {
        "device": config.DEVICE,
        "tmp_dir": config.TMP_DIR,
        "max_file_size_mb": config.MAX_FILE_SIZE // (1024 * 1024)
    }
    
    # å“åº”æ—¶é—´
    health_status["response_time_ms"] = round((time.time() - start_time) * 1000, 2)
    
    # è®¾ç½®HTTPçŠ¶æ€ç 
    status_code = 200
    if health_status["status"] == "unhealthy":
        status_code = 503
    elif health_status["status"] == "degraded":
        status_code = 200  # ä»ç„¶å¯ç”¨ï¼Œä½†æœ‰è­¦å‘Š
        
    from fastapi import status as http_status
    from fastapi.responses import JSONResponse
    
    return JSONResponse(content=health_status, status_code=status_code)

# æ·»åŠ ç»Ÿè®¡ç«¯ç‚¹
@app.get("/stats")
async def stats():
    """APIä½¿ç”¨ç»Ÿè®¡"""
    return {
        "message": "Statistics endpoint",
        "note": "Basic statistics - extend as needed",
        "uptime_seconds": time.time() - app_start_time if 'app_start_time' in globals() else 0,
        "model_loaded": model is not None,
        "config": {
            "device": config.DEVICE,
            "max_file_size_mb": config.MAX_FILE_SIZE // (1024 * 1024),
            "supported_formats": list(config.SUPPORTED_FORMATS)
        }
    }

# æ¨¡å‹åˆå§‹åŒ–å‡½æ•°ï¼Œä½¿ç”¨é…ç½®ç³»ç»Ÿ
def initialize_model():
    """
    åˆå§‹åŒ–SenseVoiceæ¨¡å‹ï¼ŒåŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶
    """
    local_model_path, local_vad_model_path = config.get_model_paths()
    
    vad_kwargs = {"max_single_segment_time": config.VAD_MAX_SEGMENT_TIME}
    model_kwargs = {
        "vad_kwargs": vad_kwargs,
        "trust_remote_code": True,
        "disable_update": True,
        "device": config.DEVICE
    }
    
    try:
        # ç­–ç•¥1: å°è¯•ä½¿ç”¨æœ¬åœ°ä¸»æ¨¡å‹å’ŒVADæ¨¡å‹
        if os.path.exists(local_model_path):
            print(f"âœ… æ‰¾åˆ°æœ¬åœ°ä¸»æ¨¡å‹: {local_model_path}")
            
            if os.path.exists(local_vad_model_path):
                print(f"âœ… æ‰¾åˆ°æœ¬åœ°VADæ¨¡å‹: {local_vad_model_path}")
                try:
                    model = AutoModel(
                        model=local_model_path,
                        vad_model=local_vad_model_path,
                        **model_kwargs
                    )
                    print("ğŸ¯ æˆåŠŸåŠ è½½ï¼šæœ¬åœ°ä¸»æ¨¡å‹ + æœ¬åœ°VADæ¨¡å‹")
                    return model
                except Exception as e:
                    print(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            
            # ç­–ç•¥2: æœ¬åœ°ä¸»æ¨¡å‹ + åœ¨çº¿VADæ¨¡å‹
            try:
                print("ğŸ”„ å°è¯•ï¼šæœ¬åœ°ä¸»æ¨¡å‹ + åœ¨çº¿VADæ¨¡å‹")
                model = AutoModel(
                    model=local_model_path,
                    vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                    **model_kwargs
                )
                print("ğŸ¯ æˆåŠŸåŠ è½½ï¼šæœ¬åœ°ä¸»æ¨¡å‹ + åœ¨çº¿VADæ¨¡å‹")
                return model
            except Exception as e:
                print(f"âš ï¸ æ··åˆæ¨¡å¼åŠ è½½å¤±è´¥: {e}")
        
        # ç­–ç•¥3: å®Œå…¨åœ¨çº¿æ¨¡å¼ï¼ˆå›é€€é€‰é¡¹ï¼‰
        print("ğŸ”„ å›é€€åˆ°å®Œå…¨åœ¨çº¿æ¨¡å¼")
        model = AutoModel(
            model="iic/SenseVoiceSmall",
            vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            **model_kwargs
        )
        print("ğŸ¯ æˆåŠŸåŠ è½½ï¼šåœ¨çº¿ä¸»æ¨¡å‹ + åœ¨çº¿VADæ¨¡å‹")
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ï¼š")
        print("   1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("   2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
        print("   3. Pythonç¯å¢ƒå’Œä¾èµ–æ˜¯å¦æ­£ç¡®")
        print(f"   4. è®¾å¤‡è®¾ç½®: {config.DEVICE}")
        raise RuntimeError(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

# è®°å½•åº”ç”¨å¯åŠ¨æ—¶é—´
import time
app_start_time = time.time()

# åˆå§‹åŒ–æ¨¡å‹
try:
    model = initialize_model()
    print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å¯åŠ¨å®Œæˆ")
except Exception as e:
    print(f"ğŸ’¥ è‡´å‘½é”™è¯¯ï¼šæ— æ³•åˆå§‹åŒ–æ¨¡å‹ - {e}")
    model = None  # è®¾ç½®ä¸ºNoneï¼Œåœ¨APIè°ƒç”¨æ—¶è¿›è¡Œæ£€æŸ¥

emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

lang_dict = {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {
    "ğŸ¼",
    "ğŸ‘",
    "ğŸ˜€",
    "ğŸ˜­",
    "ğŸ¤§",
    "ğŸ˜·",
}


def format_str_v2(text: str, show_emo=True, show_event=True):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = text.count(sptk)
        text = text.replace(sptk, "")

    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    if show_emo:
        text = text + emo_dict[emo]

    for e in event_dict:
        if sptk_dict[e] > 0 and show_event:
            text = event_dict[e] + text

    for emoji in emo_set.union(event_set):
        text = text.replace(" " + emoji, emoji)
        text = text.replace(emoji + " ", emoji)

    return text.strip()


def format_str_v3(text: str, show_emo=True, show_event=True):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None

    def get_event(s):
        return s[0] if s[0] in event_set else None

    text = text.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        text = text.replace(lang, "<|lang|>")
    parts = [format_str_v2(part, show_emo, show_event).strip(" ") for part in text.split("<|lang|>")]
    new_s = " " + parts[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(parts)):
        if len(parts[i]) == 0:
            continue
        if get_event(parts[i]) == cur_ent_event and get_event(parts[i]) is not None:
            parts[i] = parts[i][1:]
        cur_ent_event = get_event(parts[i])
        if get_emo(parts[i]) is not None and get_emo(parts[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += parts[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()


def process_audio_simple(waveform, sample_rate, target_fs=None):
    """
    ç®€åŒ–çš„éŸ³é¢‘å¤„ç†å‡½æ•°ï¼Œå‚è€ƒåŸå§‹api.pyçš„å®ç°
    
    Args:
        waveform: torch.Tensor éŸ³é¢‘æ³¢å½¢æ•°æ®
        sample_rate: int åŸå§‹é‡‡æ ·ç‡
        target_fs: int ç›®æ ‡é‡‡æ ·ç‡
        
    Returns:
        torch.Tensor: å¤„ç†åçš„éŸ³é¢‘æ•°æ®
    """
    if target_fs is None:
        target_fs = config.TARGET_SAMPLE_RATE
    
    # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if sample_rate != target_fs:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_fs)
        waveform = resampler(waveform)
    
    # å¤šé€šé“è½¬å•é€šé“ï¼ˆä½¿ç”¨mean(0)ï¼Œä¸åŸå§‹api.pyä¸€è‡´ï¼‰
    if len(waveform.shape) > 1:
        waveform = waveform.mean(0)
    
    return waveform

def model_inference(processed_audio, language, show_emo=True, show_event=True):
    """
    æ¨¡å‹æ¨ç†å‡½æ•°ï¼Œæ¥å—å·²å¤„ç†çš„éŸ³é¢‘æ•°æ®
    
    Args:
        processed_audio: torch.Tensor å·²å¤„ç†çš„éŸ³é¢‘æ•°æ®
        language: str è¯­è¨€ä»£ç 
        show_emo: bool æ˜¯å¦æ˜¾ç¤ºæƒ…æ„Ÿ
        show_event: bool æ˜¯å¦æ˜¾ç¤ºäº‹ä»¶
    """
    language = "auto" if len(language) < 1 else language

    if len(processed_audio) == 0:
        raise ValueError("The provided audio is empty.")

    merge_vad = True
    text = model.generate(
        input=processed_audio,
        cache={},
        language=language,
        use_itn=True,
        batch_size_s=0,
        merge_vad=merge_vad,
    )

    text = text[0]["text"]
    text = format_str_v3(text, show_emo, show_event)

    return text


@app.get("/v1/models")
async def models():
    """è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œå…¼å®¹OpenAI APIæ ¼å¼"""
    import time
    timestamp = int(time.time())
    
    # æ¨¡å‹çŠ¶æ€æ£€æŸ¥
    model_status = "available" if model is not None else "unavailable"
    
    return {
        "object": "list",
        "data": [
            {
                "id": "sensevoice-small",
                "object": "model", 
                "created": timestamp,
                "owned_by": "sensevoice",
                "root": "sensevoice-small",
                "parent": None,
                "status": model_status,
                "description": "SenseVoice Small - å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹",
                "context_length": 30000,  # VADæœ€å¤§æ®µé•¿åº¦
                "languages": ["auto", "zh", "en", "yue", "ja", "ko"],
                "permission": [
                    {
                        "id": "modelperm-sensevoice-001",
                        "object": "model_permission",
                        "created": timestamp,
                        "allow_create_engine": False,
                        "allow_sampling": True,
                        "allow_logprobs": True,
                        "allow_search_indices": False,
                        "allow_view": True,
                        "allow_fine_tuning": False,
                        "organization": "*",
                        "group": None,
                        "is_blocking": False
                    }
                ]
            },
            # æ·»åŠ å…¼å®¹åˆ«åï¼Œä¸åŸå§‹IDä¿æŒä¸€è‡´
            {
                "id": "iic/SenseVoiceSmall",
                "object": "model",
                "created": timestamp,
                "owned_by": "sensevoice",
                "root": "iic/SenseVoiceSmall", 
                "parent": "sensevoice-small",
                "status": model_status,
                "description": "SenseVoice Small (å…¼å®¹åˆ«å)",
                "context_length": 30000,
                "languages": ["auto", "zh", "en", "yue", "ja", "ko"],
                "permission": [
                    {
                        "id": "modelperm-sensevoice-002",
                        "object": "model_permission",
                        "created": timestamp,
                        "allow_create_engine": False,
                        "allow_sampling": True,
                        "allow_logprobs": True,
                        "allow_search_indices": False,
                        "allow_view": True,
                        "allow_fine_tuning": False,
                        "organization": "*",
                        "group": None,
                        "is_blocking": False
                    }
                ]
            }
        ]
    }


@app.post("/v1/audio/transcriptions")
async def transcriptions(
    file: Union[UploadFile, None] = File(default=None), 
    model: str = Form(default="sensevoice-small"),
    language: str = Form(default="auto"),
    prompt: str = Form(default=""),
    response_format: str = Form(default="json"),
    temperature: float = Form(default=0.0)
):
    """è½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼Œå…¼å®¹OpenAI Whisper APIæ ¼å¼"""
    
    # åŸºç¡€å‚æ•°éªŒè¯
    if file is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail={"error": {"type": "invalid_request_error", "message": "No audio file provided"}}
        )
    
    # æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={"error": {"type": "service_unavailable", "message": "Model not available, please check server logs"}}
        )

    # æ–‡ä»¶æ ¼å¼éªŒè¯
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": {"type": "invalid_request_error", "message": "Invalid filename"}}
        )
    
    # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼æ£€æŸ¥
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in config.SUPPORTED_FORMATS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"error": {"type": "invalid_request_error", "message": f"Unsupported file format: {file_ext}"}}
        )
    
    # æ–‡ä»¶å¤§å°æ£€æŸ¥
    file_content = await file.read()
    if len(file_content) > config.MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error": {"type": "invalid_request_error", "message": f"File size exceeds {config.MAX_FILE_SIZE // (1024*1024)}MB limit"}}
        )
    
    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    await file.seek(0)

    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(config.TMP_DIR, exist_ok=True)
    
    filename = file.filename
    fileobj = file.file
    # ä½¿ç”¨æ—¶é—´æˆ³å’Œè¿›ç¨‹IDç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼Œé¿å…å†²çª
    import time
    timestamp = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³
    tmp_file = Path(config.TMP_DIR) / f"upload_{timestamp}_{os.getpid()}_{filename}"

    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with open(tmp_file, "wb+") as upload_file:
            shutil.copyfileobj(fileobj, upload_file)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        if tmp_file.stat().st_size == 0:
            raise ValueError("Uploaded file is empty")
        
        try:
            # ç®€åŒ–çš„éŸ³é¢‘å¤„ç†æµç¨‹ï¼Œå‚è€ƒåŸå§‹api.py
            import gc
            
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            waveform, sample_rate = torchaudio.load(tmp_file)
            
            # ç®€å•éªŒè¯éŸ³é¢‘æ•°æ®
            if waveform.numel() == 0:
                raise ValueError("Audio file contains no data")
            
            # ä½¿ç”¨ç®€åŒ–çš„éŸ³é¢‘å¤„ç†å‡½æ•°
            processed_audio = process_audio_simple(waveform, sample_rate)
            
            # æ¸…ç†åŸå§‹éŸ³é¢‘æ•°æ®
            del waveform
            gc.collect()

            # æ‰§è¡Œæ¨ç†
            start_time = time.time()
            result = model_inference(processed_audio, language=language, show_emo=False)
            inference_time = time.time() - start_time
            
            # è®°å½•æ€§èƒ½ä¿¡æ¯
            config.logger.info(f"æ¨ç†å®Œæˆ - æ–‡ä»¶: {filename}, æ—¶é•¿: {inference_time:.3f}s, æ–‡æœ¬é•¿åº¦: {len(result)}")
            
            # æ ¹æ®response_formatè¿”å›ä¸åŒæ ¼å¼
            if response_format.lower() == "text":
                return result  # çº¯æ–‡æœ¬
            elif response_format.lower() == "srt":
                # ç®€å•çš„SRTæ ¼å¼ (æš‚æœªå®ç°æ—¶é—´æˆ³)
                return f"1\n00:00:00,000 --> 00:00:30,000\n{result}\n"
            elif response_format.lower() == "vtt":
                # WebVTTæ ¼å¼
                return f"WEBVTT\n\n1\n00:00:00.000 --> 00:00:30.000\n{result}\n"
            else:
                # é»˜è®¤JSONæ ¼å¼ï¼Œå…¼å®¹OpenAI
                response = {
                    "text": result,
                    "task": "transcribe",
                    "language": language if language != "auto" else "detected",
                    "duration": inference_time
                }
                return response
            
        except Exception as audio_error:
            # éŸ³é¢‘å¤„ç†ç›¸å…³é”™è¯¯
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={"error": {"type": "invalid_request_error", "message": f"Audio processing failed: {str(audio_error)}"}}
            )
        
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": {"type": "server_error", "message": "Failed to save uploaded file"}}
        )
    except Exception as e:
        # å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail={"error": {"type": "server_error", "message": f"Internal server error: {str(e)}"}}
        )
    finally:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        try:
            if tmp_file.exists():
                tmp_file.unlink()
        except Exception as cleanup_error:
            print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {cleanup_error}")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host=config.HOST, port=config.PORT)
