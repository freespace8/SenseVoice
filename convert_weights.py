# convert_weights.py
# å¯éªŒè¯çš„PyTorchåˆ°MLXæƒé‡è½¬æ¢è„šæœ¬

import torch
import numpy as np
from safetensors.torch import save_file
import os
from typing import Dict, Any, Tuple
from funasr import AutoModel

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹å®šä¹‰
try:
    from model import SenseVoiceSmall as SenseVoicePyTorch
except ImportError:
    print("Warning: Could not import PyTorch model directly, using AutoModel")
    SenseVoicePyTorch = None

from model_mlx import SenseVoiceMLX

print("Weight Conversion Script Initialized.")


def load_pytorch_model(model_path: str = "iic/SenseVoiceSmall"):
    """åŠ è½½PyTorchæ¨¡å‹å¹¶è·å–æƒé‡"""
    print(f"Loading PyTorch model from '{model_path}'...")
    
    # ä½¿ç”¨AutoModelåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    pytorch_model = AutoModel(
        model=model_path,
        trust_remote_code=True,
        remote_code="./model.py",
        vad_model=None,
        device="cpu",
    )
    
    # è·å–å®é™…çš„æ¨¡å‹å¯¹è±¡
    actual_model = pytorch_model.model
    actual_model.eval()
    
    print(f"PyTorch model loaded successfully.")
    return actual_model


def analyze_model_parameters(pytorch_model, mlx_model):
    """åˆ†æå’Œæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°ç»“æ„"""
    print("\n" + "="*60)
    print("PARAMETER ANALYSIS")
    print("="*60)
    
    # PyTorchæ¨¡å‹å‚æ•°
    print("\nPyTorch Model Parameters:")
    print("-"*60)
    pytorch_params = pytorch_model.state_dict()
    pytorch_param_count = 0
    pytorch_param_info = {}
    
    for name, tensor in pytorch_params.items():
        print(f"- {name:<50} | Shape: {tensor.shape}")
        pytorch_param_count += tensor.numel()
        pytorch_param_info[name] = {
            'shape': tensor.shape,
            'numel': tensor.numel(),
            'dtype': tensor.dtype
        }
    
    print(f"\nPyTorch Total Parameters: {pytorch_param_count:,}")
    
    # MLXæ¨¡å‹å‚æ•°
    print("\n" + "-"*60)
    print("MLX Model Parameters:")
    print("-"*60)
    mlx_param_count = 0
    mlx_param_info = {}
    
    # è·å–MLXæ¨¡å‹çš„æ‰€æœ‰å‚æ•°
    try:
        # MLXä½¿ç”¨ä¸åŒçš„å‚æ•°è®¿é—®æ–¹å¼
        mlx_params_dict = mlx_model.parameters()
        for name, param in mlx_params_dict.items():
            print(f"- {name:<50} | Shape: {param.shape}")
            mlx_param_count += param.size
            mlx_param_info[name] = {
                'shape': param.shape,
                'numel': param.size,
            }
    except Exception as e:
        print(f"Error accessing MLX model parameters: {e}")
        print("Attempting alternative parameter access...")
        # å°è¯•é€’å½’è·å–å‚æ•°
        def get_all_parameters(module, prefix=""):
            params = {}
            for name, value in module.__dict__.items():
                if hasattr(value, 'shape'):  # è¿™æ˜¯ä¸€ä¸ªå‚æ•°
                    full_name = f"{prefix}.{name}" if prefix else name
                    params[full_name] = value
                elif hasattr(value, '__dict__') and hasattr(value, '__class__'):
                    # è¿™æ˜¯ä¸€ä¸ªå­æ¨¡å—
                    if prefix:
                        sub_prefix = f"{prefix}.{name}"
                    else:
                        sub_prefix = name
                    sub_params = get_all_parameters(value, sub_prefix)
                    params.update(sub_params)
            return params
        
        mlx_params_dict = get_all_parameters(mlx_model)
        for name, param in mlx_params_dict.items():
            if hasattr(param, 'shape'):
                print(f"- {name:<50} | Shape: {param.shape}")
                mlx_param_count += param.size
                mlx_param_info[name] = {
                    'shape': param.shape,
                    'numel': param.size,
                }
    
    print(f"\nMLX Total Parameters: {mlx_param_count:,}")
    
    # å‚æ•°æ•°é‡å¯¹æ¯”
    print("\n" + "="*60)
    print("PARAMETER COUNT COMPARISON")
    print("="*60)
    print(f"PyTorch: {pytorch_param_count:,}")
    print(f"MLX:     {mlx_param_count:,}")
    print(f"Difference: {abs(pytorch_param_count - mlx_param_count):,}")
    
    if pytorch_param_count != mlx_param_count:
        print("âš ï¸  WARNING: Parameter counts don't match!")
    else:
        print("âœ… Parameter counts match!")
    
    return pytorch_param_info, mlx_param_info


def create_parameter_mapping(pytorch_params: Dict, mlx_params: Dict) -> Dict[str, str]:
    """åˆ›å»ºPyTorchåˆ°MLXå‚æ•°åç§°çš„æ˜ å°„"""
    print("\n" + "="*60)
    print("CREATING PARAMETER MAPPING")
    print("="*60)
    
    mapping = {}
    unmapped_pytorch = set(pytorch_params.keys())
    unmapped_mlx = set(mlx_params.keys())
    
    # ç›´æ¥åŒ¹é…ï¼ˆåç§°å®Œå…¨ç›¸åŒï¼‰
    print("\nDirect matches:")
    for pt_name in list(unmapped_pytorch):
        if pt_name in unmapped_mlx:
            mapping[pt_name] = pt_name
            unmapped_pytorch.remove(pt_name)
            unmapped_mlx.remove(pt_name)
            print(f"âœ… {pt_name}")
    
    # æ¨¡å¼åŒ¹é…
    print("\nPattern-based mapping:")
    
    # Embeddingå±‚æ˜ å°„
    for pt_name in list(unmapped_pytorch):
        if 'embed.weight' in pt_name:
            for mlx_name in list(unmapped_mlx):
                if 'embed.weight' in mlx_name:
                    mapping[pt_name] = mlx_name
                    unmapped_pytorch.remove(pt_name)
                    unmapped_mlx.remove(mlx_name)
                    print(f"ğŸ”„ {pt_name} -> {mlx_name}")
                    break
    
    # CTCå±‚æ˜ å°„
    for pt_name in list(unmapped_pytorch):
        if 'ctc.ctc_lo' in pt_name:
            for mlx_name in list(unmapped_mlx):
                if 'ctc.ctc_lo' in mlx_name:
                    mapping[pt_name] = mlx_name
                    unmapped_pytorch.remove(pt_name)
                    unmapped_mlx.remove(mlx_name)
                    print(f"ğŸ”„ {pt_name} -> {mlx_name}")
                    break
    
    # ç¼–ç å™¨å±‚æ˜ å°„ - å¤æ‚çš„å±‚çº§ç»“æ„
    encoder_mappings = [
        ('encoders.', 'encoders.'),
        ('self_attn.linear_q_k_v', 'self_attn.linear_q_k_v'),
        ('self_attn.linear_out', 'self_attn.linear_out'),
        ('self_attn.fsmn_block', 'self_attn.fsmn_conv'),
        ('feed_forward.w_1', 'feed_forward.w_1'),
        ('feed_forward.w_2', 'feed_forward.w_2'),
        ('norm1', 'norm1'),
        ('norm2', 'norm2'),
    ]
    
    for pt_name in list(unmapped_pytorch):
        for mlx_name in list(unmapped_mlx):
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¼–ç å™¨å‚æ•°
            if 'encoder' in pt_name.lower() and 'encoder' in mlx_name.lower():
                # å°è¯•åŒ¹é…ç¼–ç å™¨å±‚
                for pt_pattern, mlx_pattern in encoder_mappings:
                    if pt_pattern in pt_name and mlx_pattern in mlx_name:
                        # æå–å±‚å·
                        try:
                            pt_parts = pt_name.split('.')
                            mlx_parts = mlx_name.split('.')
                            
                            # æ‰¾åˆ°å±‚å·ä½ç½®
                            pt_layer_idx = None
                            mlx_layer_idx = None
                            
                            for i, part in enumerate(pt_parts):
                                if part.isdigit():
                                    pt_layer_idx = int(part)
                                    break
                            
                            for i, part in enumerate(mlx_parts):
                                if part.isdigit():
                                    mlx_layer_idx = int(part)
                                    break
                            
                            # å¦‚æœå±‚å·åŒ¹é…ï¼Œè¿›è¡Œæ˜ å°„
                            if pt_layer_idx is not None and mlx_layer_idx is not None and pt_layer_idx == mlx_layer_idx:
                                # è¿›ä¸€æ­¥æ£€æŸ¥å‚æ•°ç±»å‹æ˜¯å¦åŒ¹é…
                                pt_suffix = pt_name.split('.')[-1]  # weight æˆ– bias
                                mlx_suffix = mlx_name.split('.')[-1]
                                
                                if pt_suffix == mlx_suffix:
                                    mapping[pt_name] = mlx_name
                                    unmapped_pytorch.remove(pt_name)
                                    unmapped_mlx.remove(mlx_name)
                                    print(f"ğŸ”„ {pt_name} -> {mlx_name}")
                                    break
                        except (IndexError, ValueError):
                            continue
            
            if pt_name not in unmapped_pytorch:
                break
    
    # æŠ¥å‘Šæœªæ˜ å°„çš„å‚æ•°
    print(f"\nğŸ“Š Mapping Summary:")
    print(f"   Mapped: {len(mapping)}")
    print(f"   Unmapped PyTorch: {len(unmapped_pytorch)}")
    print(f"   Unmapped MLX: {len(unmapped_mlx)}")
    
    if unmapped_pytorch:
        print(f"\nâš ï¸  Unmapped PyTorch parameters:")
        for name in sorted(unmapped_pytorch):
            print(f"   - {name}")
    
    if unmapped_mlx:
        print(f"\nâš ï¸  Unmapped MLX parameters:")
        for name in sorted(unmapped_mlx):
            print(f"   - {name}")
    
    return mapping


def convert_weight_tensor(name: str, tensor: torch.Tensor) -> np.ndarray:
    """è½¬æ¢å•ä¸ªæƒé‡å¼ é‡çš„æ ¼å¼"""
    numpy_tensor = tensor.detach().cpu().numpy()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬ç½®ï¼ˆçº¿æ€§å±‚æƒé‡ï¼‰
    if 'linear' in name.lower() and 'weight' in name and len(numpy_tensor.shape) == 2:
        # PyTorch Linear: (out_features, in_features)
        # MLX Linear: (out_features, in_features) - å®é™…ä¸ŠMLXä¹Ÿæ˜¯è¿™ä¸ªé¡ºåº
        # ä½†æ ¹æ®Geminiçš„å»ºè®®ï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯è¿™ä¸€ç‚¹
        print(f"   Linear weight: {name} | Shape: {numpy_tensor.shape}")
        # æš‚æ—¶ä¸è½¬ç½®ï¼Œç­‰å¾…éªŒè¯
        pass
    
    elif 'conv' in name.lower() and 'weight' in name:
        # å·ç§¯å±‚æƒé‡æ ¼å¼æ£€æŸ¥
        print(f"   Conv weight: {name} | Shape: {numpy_tensor.shape}")
        pass
    
    return numpy_tensor


def convert_weights(pytorch_model, mlx_model, mapping: Dict[str, str]) -> Dict[str, torch.Tensor]:
    """æ‰§è¡Œæƒé‡è½¬æ¢"""
    print("\n" + "="*60)
    print("WEIGHT CONVERSION")
    print("="*60)
    
    pytorch_state_dict = pytorch_model.state_dict()
    converted_weights = {}
    
    conversion_stats = {
        'successful': 0,
        'failed': 0,
        'shape_mismatches': 0
    }
    
    for pt_name, mlx_name in mapping.items():
        if pt_name in pytorch_state_dict:
            try:
                # è·å–PyTorchæƒé‡
                pt_tensor = pytorch_state_dict[pt_name]
                
                # è½¬æ¢æ ¼å¼
                converted_tensor = convert_weight_tensor(pt_name, pt_tensor)
                
                # è½¬æ¢å›torchå¼ é‡ç”¨äºsafetensorsä¿å­˜
                converted_weights[mlx_name] = torch.from_numpy(converted_tensor)
                
                print(f"âœ… {pt_name} -> {mlx_name} | Shape: {pt_tensor.shape}")
                conversion_stats['successful'] += 1
                
            except Exception as e:
                print(f"âŒ Failed to convert {pt_name}: {e}")
                conversion_stats['failed'] += 1
        else:
            print(f"âš ï¸  PyTorch parameter not found: {pt_name}")
            conversion_stats['failed'] += 1
    
    print(f"\nğŸ“Š Conversion Summary:")
    print(f"   Successful: {conversion_stats['successful']}")
    print(f"   Failed: {conversion_stats['failed']}")
    print(f"   Shape mismatches: {conversion_stats['shape_mismatches']}")
    
    return converted_weights


def save_converted_weights(weights: Dict[str, torch.Tensor], output_path: str):
    """ä¿å­˜è½¬æ¢åçš„æƒé‡åˆ°safetensorsæ–‡ä»¶"""
    print(f"\nğŸ’¾ Saving converted weights to '{output_path}'...")
    
    try:
        save_file(weights, output_path)
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"âœ… Successfully saved {len(weights)} parameters to {output_path}")
        print(f"   File size: {file_size:.2f} MB")
        return True
    except Exception as e:
        print(f"âŒ Failed to save weights: {e}")
        return False


def verify_conversion(original_model, converted_weights: Dict[str, torch.Tensor], mapping: Dict[str, str]):
    """éªŒè¯è½¬æ¢çš„æ­£ç¡®æ€§"""
    print("\n" + "="*60)
    print("CONVERSION VERIFICATION")
    print("="*60)
    
    pytorch_state_dict = original_model.state_dict()
    verification_results = []
    
    for pt_name, mlx_name in mapping.items():
        if pt_name in pytorch_state_dict and mlx_name in converted_weights:
            pt_tensor = pytorch_state_dict[pt_name]
            converted_tensor = converted_weights[mlx_name]
            
            # æ¯”è¾ƒå½¢çŠ¶
            shape_match = pt_tensor.shape == converted_tensor.shape
            
            # æ¯”è¾ƒæ•°å€¼ï¼ˆå…è®¸å°çš„æµ®ç‚¹è¯¯å·®ï¼‰
            if shape_match:
                max_diff = torch.max(torch.abs(pt_tensor - converted_tensor)).item()
                numerical_match = max_diff < 1e-6
            else:
                max_diff = float('inf')
                numerical_match = False
            
            verification_results.append({
                'pt_name': pt_name,
                'mlx_name': mlx_name,
                'shape_match': shape_match,
                'numerical_match': numerical_match,
                'max_diff': max_diff
            })
            
            status = "âœ…" if shape_match and numerical_match else "âŒ"
            print(f"{status} {pt_name} -> {mlx_name}")
            if not shape_match:
                print(f"   Shape mismatch: {pt_tensor.shape} vs {converted_tensor.shape}")
            if not numerical_match:
                print(f"   Numerical difference: {max_diff}")
    
    # ç»Ÿè®¡éªŒè¯ç»“æœ
    total = len(verification_results)
    shape_matches = sum(1 for r in verification_results if r['shape_match'])
    numerical_matches = sum(1 for r in verification_results if r['numerical_match'])
    
    print(f"\nğŸ“Š Verification Summary:")
    print(f"   Total parameters: {total}")
    print(f"   Shape matches: {shape_matches}/{total} ({100*shape_matches/total:.1f}%)")
    print(f"   Numerical matches: {numerical_matches}/{total} ({100*numerical_matches/total:.1f}%)")
    
    return verification_results


def main(model_path: str = "iic/SenseVoiceSmall", output_path: str = "sensevoice_mlx.safetensors"):
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æƒé‡è½¬æ¢æµç¨‹"""
    print("ğŸš€ Starting PyTorch to MLX weight conversion...")
    print(f"   Model path: {model_path}")
    print(f"   Output path: {output_path}")
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        pytorch_model = load_pytorch_model(model_path)
        mlx_model = SenseVoiceMLX()
        
        # 2. åˆ†æå‚æ•°ç»“æ„
        pytorch_params, mlx_params = analyze_model_parameters(pytorch_model, mlx_model)
        
        # 3. åˆ›å»ºå‚æ•°æ˜ å°„
        mapping = create_parameter_mapping(pytorch_params, mlx_params)
        
        if not mapping:
            print("âŒ No parameter mappings found! Check model architectures.")
            return False
        
        # 4. è½¬æ¢æƒé‡
        converted_weights = convert_weights(pytorch_model, mlx_model, mapping)
        
        if not converted_weights:
            print("âŒ No weights converted! Check parameter mapping.")
            return False
        
        # 5. éªŒè¯è½¬æ¢
        verification_results = verify_conversion(pytorch_model, converted_weights, mapping)
        
        # 6. ä¿å­˜æƒé‡
        success = save_converted_weights(converted_weights, output_path)
        
        if success:
            print(f"\nğŸ‰ Weight conversion completed successfully!")
            print(f"   Output file: {output_path}")
            print(f"   Converted parameters: {len(converted_weights)}")
            return True
        else:
            print(f"\nâŒ Weight conversion failed during saving.")
            return False
            
    except Exception as e:
        print(f"âŒ Weight conversion failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Convert PyTorch SenseVoice weights to MLX format")
    parser.add_argument("--model_path", type=str, default="iic/SenseVoiceSmall", 
                       help="Path to PyTorch model")
    parser.add_argument("--output_path", type=str, default="sensevoice_mlx.safetensors",
                       help="Output path for converted weights")
    
    args = parser.parse_args()
    
    success = main(args.model_path, args.output_path)
    if success:
        print("\nâœ… Conversion script completed successfully.")
        exit(0)
    else:
        print("\nâŒ Conversion script failed.")
        exit(1)