#!/usr/bin/env python3
"""
SenseVoice æ¨¡å‹å¯¹æ¯”å·¥å…· - PyTorch vs MLX
æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œè¾“å‡ºç»“æœ
"""

import os
import sys
import time
import torch
import numpy as np
import librosa
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.align import Align

# è®¾ç½® CUDA è®¾å¤‡ä¸º CPU (Mac ç¯å¢ƒ)
os.environ['CUDA_VISIBLE_DEVICES'] = ''
torch.set_default_tensor_type('torch.FloatTensor')

# å¯¼å…¥ MLX ç›¸å…³
import mlx.core as mx

# å¯¼å…¥æ¨¡å‹
from model import SenseVoiceSmall
from model_mlx import SenseVoiceMLX
from utils.frontend_mlx import create_frontend_mlx

# å¯¼å…¥ FunASR
from funasr import AutoModel


def load_pytorch_model(model_dir="/Users/taylor/.cache/modelscope/hub/models/iic/SenseVoiceSmall"):
    """åŠ è½½ PyTorch æ¨¡å‹"""
    print("\nğŸ“¦ åŠ è½½ PyTorch æ¨¡å‹...")
    start_time = time.time()
    
    # ä½¿ç”¨ from_pretrained æ–¹æ³•åŠ è½½
    try:
        # å¼ºåˆ¶ä½¿ç”¨ CPU
        model, kwargs = SenseVoiceSmall.from_pretrained(
            model=model_dir, 
            device="cpu",
            disable_update=True,
            disable_log=True
        )
        model.eval()
        load_time = time.time() - start_time
        print(f"   âœ… PyTorch æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}ç§’)")
        return model, kwargs
    except Exception as e:
        print(f"   âŒ PyTorch æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None


def load_mlx_model(model_path="/Users/taylor/Documents/code/SenseVoice/model/model_mlx.safetensors"):
    """åŠ è½½ MLX æ¨¡å‹"""
    print("\nğŸ“¦ åŠ è½½ MLX æ¨¡å‹...")
    start_time = time.time()
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = SenseVoiceMLX(
            input_size=560,  # LFR ç‰¹å¾ç»´åº¦
            vocab_size=25055,
            encoder_conf={
                "output_size": 512,
                "attention_heads": 4,
                "linear_units": 2048,
                "num_blocks": 50,
                "tp_blocks": 20,
                "dropout_rate": 0.1,
                "positional_dropout_rate": 0.1,
                "attention_dropout_rate": 0.0,
                "normalize_before": True,
                "kernel_size": 11,
                "sanm_shift": 0,
            }
        )
        
        # åŠ è½½æƒé‡
        weights = mx.load(model_path)
        model.load_weights(weights)
        
        load_time = time.time() - start_time
        print(f"   âœ… MLX æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}ç§’)")
        return model
    except Exception as e:
        print(f"   âŒ MLX æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


def extract_features_for_pytorch(audio_path, frontend):
    """ä¸º PyTorch æ¨¡å‹æå–ç‰¹å¾"""
    # è¯»å–éŸ³é¢‘
    waveform, sr = librosa.load(audio_path, sr=16000, mono=True)
    
    # ä½¿ç”¨ç»Ÿä¸€å‰ç«¯æå–ç‰¹å¾
    features = frontend(waveform, output_format="numpy")
    
    # è½¬æ¢ä¸º PyTorch å¼ é‡
    features_pt = torch.from_numpy(features).float()
    
    # æ·»åŠ  batch ç»´åº¦
    if len(features_pt.shape) == 2:
        features_pt = features_pt.unsqueeze(0)
    
    return features_pt, features_pt.shape[1]


def extract_features_for_mlx(audio_path, frontend):
    """ä¸º MLX æ¨¡å‹æå–ç‰¹å¾"""
    # è¯»å–éŸ³é¢‘
    waveform, sr = librosa.load(audio_path, sr=16000, mono=True)
    
    # ä½¿ç”¨ç»Ÿä¸€å‰ç«¯æå–ç‰¹å¾ï¼Œç›´æ¥è¿”å› MLX æ ¼å¼
    features_mx = frontend(waveform, output_format="mlx")
    
    return features_mx, features_mx.shape[1]


def inference_pytorch(model, kwargs, audio_path, language="auto"):
    """ä½¿ç”¨ PyTorch æ¨¡å‹è¿›è¡Œæ¨ç†"""
    print(f"\nğŸ”¥ PyTorch æ¨ç†: {audio_path}")
    
    try:
        # ä½¿ç”¨æ¨¡å‹çš„ inference æ–¹æ³•
        start_time = time.time()
        
        res = model.inference(
            data_in=audio_path,
            language=language,
            use_itn=False,
            ban_emo_unk=False,
            **kwargs
        )
        
        inference_time = time.time() - start_time
        
        if res and len(res) > 0:
            text = res[0][0]['text'] if isinstance(res[0], list) else res[0]['text']
            print(f"   â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
            print(f"   ğŸ“ è¯†åˆ«ç»“æœ: {text}")
            return text, inference_time
        else:
            print(f"   âŒ æ— è¯†åˆ«ç»“æœ")
            return "[æ— è¯†åˆ«ç»“æœ]", inference_time
            
    except Exception as e:
        print(f"   âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return f"[é”™è¯¯: {str(e)}]", 0


def load_tokenizer():
    """åŠ è½½ tokenizer ç”¨äºè§£ç """
    try:
        # é¦–å…ˆå°è¯•åŠ è½½ sentencepiece tokenizer
        from funasr.tokenizer.sentencepiece_tokenizer import SentencepiecesTokenizer
        model_dir = "/Users/taylor/.cache/modelscope/hub/models/iic/SenseVoiceSmall"
        tokenizer_conf = {"sentencepiece_model": f"{model_dir}/chn_jpn_yue_eng_ko_spectok.bpe.model"}
        tokenizer = SentencepiecesTokenizer(**tokenizer_conf)
        return tokenizer
    except:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ tokens.json
        try:
            import json
            tokens_file = "/Users/taylor/.cache/modelscope/hub/models/iic/SenseVoiceSmall/tokens.json"
            with open(tokens_file, 'r', encoding='utf-8') as f:
                tokens = json.load(f)
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„ tokenizer å¯¹è±¡
            class SimpleTokenizer:
                def __init__(self, tokens):
                    self.tokens = tokens
                    self.id2token = {i: token for i, token in enumerate(tokens)}
                
                def decode(self, token_ids, keep_special_tokens=False):
                    """å°† token IDs è§£ç ä¸ºæ–‡æœ¬
                    
                    Args:
                        token_ids: Token ID åˆ—è¡¨
                        keep_special_tokens: æ˜¯å¦ä¿ç•™ç‰¹æ®Šæ ‡è®°
                    """
                    text_tokens = []
                    for tid in token_ids:
                        if tid in self.id2token:
                            token = self.id2token[tid]
                            # æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦è·³è¿‡ç‰¹æ®Šæ ‡è®°
                            if keep_special_tokens or not (token.startswith('<') and token.endswith('>')):
                                text_tokens.append(token)
                    
                    # åˆå¹¶æ–‡æœ¬
                    text = ''.join(text_tokens)
                    # æ¸…ç†æ–‡æœ¬
                    text = text.replace('â–', ' ')  # æ›¿æ¢ç©ºæ ¼æ ‡è®°
                    text = text.strip()
                    return text
            
            return SimpleTokenizer(tokens)
        except:
            return None


def inference_mlx(model, frontend, audio_path, language="auto", tokenizer=None, keep_special_tokens=False):
    """ä½¿ç”¨ MLX æ¨¡å‹è¿›è¡Œæ¨ç†
    
    Args:
        model: MLX æ¨¡å‹
        frontend: å‰ç«¯å¤„ç†å™¨
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        language: è¯­è¨€è®¾ç½®
        tokenizer: åˆ†è¯å™¨
        keep_special_tokens: æ˜¯å¦ä¿ç•™ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚è¯­è¨€ã€æƒ…æ„Ÿæ ‡è®°ï¼‰
    """
    print(f"\nâš¡ MLX æ¨ç†: {audio_path}")
    
    try:
        # æå–ç‰¹å¾
        features_mx = extract_features_for_mlx(audio_path, frontend)
        speech_lengths = mx.array([features_mx[0].shape[1]], dtype=mx.int32)
        
        # æ¨ç†
        start_time = time.time()
        outputs = model(features_mx[0], speech_lengths)
        ctc_logits = outputs['ctc_logits']
        
        # CTC è§£ç 
        token_ids = mx.argmax(ctc_logits[0], axis=-1)
        
        # å»é™¤é‡å¤
        token_ids_np = np.array(token_ids)
        decoded_tokens = []
        prev_token = -1
        for token in token_ids_np:
            if token != prev_token and token != 0:  # 0 æ˜¯ blank token
                decoded_tokens.append(token)
            prev_token = token
        
        inference_time = time.time() - start_time
        
        # ä½¿ç”¨ tokenizer è§£ç æ–‡æœ¬
        if tokenizer and len(decoded_tokens) > 0:
            try:
                text = tokenizer.decode(decoded_tokens, keep_special_tokens=keep_special_tokens)
            except:
                text = f"[Tokens: {decoded_tokens[:20]}...]" if len(decoded_tokens) > 20 else f"[Tokens: {decoded_tokens}]"
        else:
            text = f"[Tokens: {decoded_tokens[:20]}...]" if len(decoded_tokens) > 20 else f"[Tokens: {decoded_tokens}]"
        
        print(f"   â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
        print(f"   ğŸ“ è¯†åˆ«ç»“æœ: {text}")
        return text, inference_time, decoded_tokens
        
    except Exception as e:
        print(f"   âŒ æ¨ç†å¤±è´¥: {e}")
        return f"[é”™è¯¯: {str(e)}]", 0, []


def calculate_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    try:
        from difflib import SequenceMatcher
        
        # æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤ç‰¹æ®Šæ ‡è®°
        def clean_text(text):
            # å»é™¤ç‰¹æ®Šæ ‡è®°
            import re
            text = re.sub(r'<\|[^|]+\|>', '', text)
            text = text.strip()
            return text
        
        clean1 = clean_text(text1)
        clean2 = clean_text(text2)
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªæ˜¯ token åˆ—è¡¨ï¼Œè¿”å› 0
        if "[Tokens:" in text2 or "[é”™è¯¯:" in text1 or "[é”™è¯¯:" in text2:
            return 0.0
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, clean1, clean2).ratio()
        return similarity * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    except:
        return 0.0


def compare_models(audio_files):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ SenseVoice æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    pytorch_model, pytorch_kwargs = load_pytorch_model()
    mlx_model = load_mlx_model()
    
    if not pytorch_model or not mlx_model:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
        return
    
    # åˆ›å»ºç»Ÿä¸€å‰ç«¯
    print("\nğŸ“ åˆå§‹åŒ–ç»Ÿä¸€å‰ç«¯...")
    cmvn_file = "/Users/taylor/.cache/modelscope/hub/models/iic/SenseVoiceSmall/am.mvn"
    frontend = create_frontend_mlx(cmvn_file=cmvn_file if os.path.exists(cmvn_file) else None)
    
    # åŠ è½½ tokenizer
    print("ğŸ“– åŠ è½½ Tokenizer...")
    tokenizer = load_tokenizer()
    if tokenizer:
        print("   âœ… Tokenizer åŠ è½½æˆåŠŸ")
    else:
        print("   âš ï¸  Tokenizer åŠ è½½å¤±è´¥ï¼ŒMLX å°†åªè¾“å‡º Token IDs")
    
    # å¯¹æ¯”ç»“æœ
    results = []
    
    for audio_file in audio_files:
        print("\n" + "-" * 80)
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(audio_file)}")
        
        # ä»æ–‡ä»¶åæ¨æ–­è¯­è¨€
        filename = os.path.basename(audio_file)
        if filename.startswith('zh'):
            language = 'zh'
        elif filename.startswith('en'):
            language = 'en'
        elif filename.startswith('ja'):
            language = 'ja'
        elif filename.startswith('ko'):
            language = 'ko'
        elif filename.startswith('yue'):
            language = 'yue'
        else:
            language = 'auto'
        
        print(f"   ğŸŒ è¯­è¨€: {language}")
        
        # PyTorch æ¨ç†
        pytorch_text, pytorch_time = inference_pytorch(
            pytorch_model, pytorch_kwargs, audio_file, language
        )
        
        # MLX æ¨ç†ï¼ˆä¿ç•™ç‰¹æ®Šæ ‡è®°ä»¥åŒ¹é… PyTorch è¾“å‡ºæ ¼å¼ï¼‰
        mlx_text, mlx_time, mlx_tokens = inference_mlx(
            mlx_model, frontend, audio_file, language, tokenizer, keep_special_tokens=True
        )
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if pytorch_time > 0 and mlx_time > 0:
            speedup = pytorch_time / mlx_time
            print(f"\n   âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        else:
            speedup = 0
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = calculate_similarity(pytorch_text, mlx_text)
        print(f"   ğŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity:.1f}%")
        
        results.append({
            'file': os.path.basename(audio_file),
            'language': language,
            'pytorch_text': pytorch_text,
            'pytorch_time': pytorch_time,
            'mlx_text': mlx_text,
            'mlx_time': mlx_time,
            'speedup': speedup,
            'similarity': similarity,
            'mlx_tokens': mlx_tokens
        })
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")
    print("=" * 80)
    
    total_pytorch_time = sum(r['pytorch_time'] for r in results)
    total_mlx_time = sum(r['mlx_time'] for r in results)
    avg_similarity = sum(r['similarity'] for r in results) / len(results) if results else 0
    avg_speedup = total_pytorch_time/total_mlx_time if total_mlx_time > 0 else 0
    
    # åˆ›å»º Rich è¡¨æ ¼
    console = Console()
    table = Table(title="", show_header=True, header_style="bold cyan")
    
    # æ·»åŠ åˆ—
    table.add_column("æ–‡ä»¶", style="yellow", no_wrap=True, width=12)
    table.add_column("PyTorch(s)", justify="right", style="red", width=12)
    table.add_column("MLX(s)", justify="right", style="green", width=10)
    table.add_column("åŠ é€Ÿæ¯”", justify="right", style="magenta", width=10)
    table.add_column("ç›¸ä¼¼åº¦", justify="right", style="blue", width=10)
    
    # æ·»åŠ æ•°æ®è¡Œ
    for r in results:
        table.add_row(
            r['file'],
            f"{r['pytorch_time']:.3f}",
            f"{r['mlx_time']:.3f}",
            f"{r['speedup']:.2f}x",
            f"{r['similarity']:.1f}%"
        )
    
    # æ·»åŠ åˆ†éš”çº¿
    table.add_section()
    
    # æ·»åŠ æ±‡æ€»è¡Œ
    table.add_row(
        "æ€»è®¡/å¹³å‡",
        f"{total_pytorch_time:.3f}",
        f"{total_mlx_time:.3f}",
        f"{avg_speedup:.2f}x",
        f"{avg_similarity:.1f}%",
        style="bold"
    )
    
    # æ‰“å°è¡¨æ ¼
    print()
    console.print(table)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "model_comparison_results.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("SenseVoice æ¨¡å‹å¯¹æ¯”ç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        
        for r in results:
            f.write(f"æ–‡ä»¶: {r['file']}\n")
            f.write(f"è¯­è¨€: {r['language']}\n")
            f.write(f"PyTorch ç»“æœ: {r['pytorch_text']}\n")
            f.write(f"PyTorch æ—¶é—´: {r['pytorch_time']:.3f}ç§’\n")
            f.write(f"MLX ç»“æœ: {r['mlx_text']}\n")
            f.write(f"MLX æ—¶é—´: {r['mlx_time']:.3f}ç§’\n")
            f.write(f"åŠ é€Ÿæ¯”: {r['speedup']:.2f}x\n")
            f.write(f"æ–‡æœ¬ç›¸ä¼¼åº¦: {r['similarity']:.1f}%\n")
            if 'mlx_tokens' in r and r['mlx_tokens']:
                f.write(f"MLX Tokens (å‰20ä¸ª): {r['mlx_tokens'][:20]}\n")
            f.write("-" * 40 + "\n\n")
        
        f.write(f"\næ€»è®¡:\n")
        f.write(f"PyTorch æ€»æ—¶é—´: {total_pytorch_time:.3f}ç§’\n")
        f.write(f"MLX æ€»æ—¶é—´: {total_mlx_time:.3f}ç§’\n")
        f.write(f"å¹³å‡åŠ é€Ÿæ¯”: {total_pytorch_time/total_mlx_time if total_mlx_time > 0 else 0:.2f}x\n")
        f.write(f"å¹³å‡æ–‡æœ¬ç›¸ä¼¼åº¦: {avg_similarity:.1f}%\n")
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    # ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶
    examples_dir = "/Users/taylor/Documents/code/SenseVoice/examples"
    
    if not os.path.exists(examples_dir):
        print(f"âŒ ç¤ºä¾‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {examples_dir}")
        return
    
    # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    audio_files = sorted([
        os.path.join(examples_dir, f) 
        for f in os.listdir(examples_dir) 
        if f.endswith('.mp3')
    ])
    
    if not audio_files:
        print(f"âŒ åœ¨ {examples_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # è¿›è¡Œå¯¹æ¯”
    compare_models(audio_files)


if __name__ == "__main__":
    main()